{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vae'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5a617c5815f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvae\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConvAEDeep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m '''\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'vae'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Jul 11 07:32:54 2019\n",
    "\n",
    "@author: dudley\n",
    "\"\"\"\n",
    "\n",
    "from itertools import count\n",
    "\n",
    "from heapq import heappush, heappop\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from scipy.signal import argrelextrema\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.feature_extraction.image import extract_patches\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "\n",
    "from vae import ConvAEDeep\n",
    "\n",
    "'''\n",
    "The AutoWellCorrelation class is intended to automate the well log correlation \n",
    "process using an adapted 1D version of panoramic stiching commonly utilitized \n",
    "in image processing to merge two or more overlapping images.  This class will load \n",
    "a pre generated project, load a PyTorch model, process the log data, identify \n",
    "reasonable well pairs, and then build a connectivity graph based on key points\n",
    "that were identified using a 1D SIFT(ish) workflow.  This connectivity graph can\n",
    "then be queried to find the most likely corresponding depths between all wells.\n",
    "\n",
    "This class also provides a series of utility functions to preview data to ensure\n",
    "quality control.\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "class AutoWellCorrelation:\n",
    "    \n",
    "    def __init__(self, projPath, modelPath, resampleInc=2, minZ=None, maxZ=None,\n",
    "                 maxOffset=1000, smSigma=5, numNeighbors=6):\n",
    "        \n",
    "        self.projPath = projPath\n",
    "        self.modelPath = modelPath\n",
    "        self.resampleInce = resampleInc\n",
    "        self.minZ = minZ\n",
    "        self.maxZ = maxZ\n",
    "        self.maxOffset = maxOffset\n",
    "        self.smSigma = smSigma\n",
    "        self.numNeighbors = numNeighbors\n",
    "        self.windowSize = 256\n",
    "        self.halfWindow = self.windowSize // 2\n",
    "        \n",
    "        self.getModel()\n",
    "        self.getWellData()   \n",
    "        self.getWellPairs()\n",
    "        self.scaleLogData()\n",
    "        self.computePatches()\n",
    "        \n",
    "        \n",
    "    # Create a NetworkX graph to store likely correlations between all wells \n",
    "    # across all depths\n",
    "    def buildConnectivityGraph(self):\n",
    "        \n",
    "        # Define the weighted graph\n",
    "        self.G = nx.Graph()\n",
    "        \n",
    "        # Iterate over all well pairs (u & v are well identifiers, w is distance)\n",
    "        for u,v,w in self.wellPairs.edges(data=True):\n",
    "            try:\n",
    "                \n",
    "                # Get necessary data and assign to variables\n",
    "                uid1 = u\n",
    "                uid2 = v\n",
    "                tvd1 = self.logData[self.logData['uid'] == uid1].index.values\n",
    "                tvd2 = self.logData[self.logData['uid'] == uid2].index.values\n",
    "                \n",
    "                # Create feature vectors\n",
    "                vector1 = self.getFeatureVector(uid1)\n",
    "                vector2 = self.getFeatureVector(uid2)\n",
    "                \n",
    "                # Compute DoG and Key Points for each well in pair\n",
    "                dog1 = self.computeDoG(uid1)\n",
    "                dog2 = self.computeDoG(uid2)\n",
    "                kP1 = self.getKeyPoints(dog1)                \n",
    "                kP2 = self.getKeyPoints(dog2)\n",
    "                \n",
    "                # Compute vector and depth distance matrices\n",
    "                vectorDist = self.computeVectorDistance(vector1, vector2, kP1, kP2)\n",
    "                tvdDist = self.computeTvdDistance(tvd1, tvd2, kP1, kP2)\n",
    "                \n",
    "                # Mask the distance matrices based on maximum allowable offset \n",
    "                # between well pair\n",
    "                vectorMask, tvdMask = self.computeVectorMask(vectorDist, tvdDist)\n",
    "                \n",
    "                # Identify matching points and retrieve weights at those locations\n",
    "                mPs = self.getMatchPoints(vectorMask)\n",
    "                weights = vectorMask[mPs[:, 1], mPs[:, 0]]\n",
    "                \n",
    "                # Iterate over matching points, get depth value and add to Graph\n",
    "                for num, mP in enumerate(mPs):\n",
    "                    n1 = (uid1, tvd1[kP1[mP[0]]])\n",
    "                    n2 = (uid2, tvd2[kP2[mP[1]]])                    \n",
    "                    self.G.add_edge(n1, n2, weight=weights[num])\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        \n",
    "        \n",
    "    # Compute Difference of Gaussians\n",
    "    def computeDoG(self, uid, k=1.6, sigmaFactor=2):\n",
    "        \n",
    "        # Retrieve log name from data frame\n",
    "        logName = self.logData.columns[0]\n",
    "        \n",
    "        # Get log data for specified well identifier\n",
    "        logData = self.logData[self.logData['uid'] == uid][logName].values\n",
    "        \n",
    "        # Create placeholder for values\n",
    "        dog = np.zeros((logData.size, 4))\n",
    "        \n",
    "        # Iterate over sigma values, compute filtered values, generate difference\n",
    "        for idx, sigma in enumerate(np.arange(1, 9, 2) * sigmaFactor):\n",
    "            f1 = gaussian_filter(logData, sigma)\n",
    "            f2 = gaussian_filter(logData, sigma * k)\n",
    "            dog[:, idx] = f1 - f2\n",
    "            \n",
    "        return dog\n",
    "    \n",
    "    \n",
    "    # Generate patches for each well\n",
    "    def computePatches(self):\n",
    "        \n",
    "        # Retrieve log name from data frame\n",
    "        logName = self.logData.columns[0]\n",
    "        \n",
    "        # Iterate over well identifiers, pad data, extract patches, add to dict\n",
    "        self.patches = {}\n",
    "        for uid in self.logData['uid'].unique():\n",
    "            data = self.logData[self.logData['uid'] == uid]\n",
    "            log = data[logName].values\n",
    "            pad = np.pad(log, (self.halfWindow, self.halfWindow), mode='reflect')\n",
    "            patch = extract_patches(pad, self.windowSize)\n",
    "            patch = patch[np.newaxis]\n",
    "            patch= np.moveaxis(patch, 1, 0)            \n",
    "            self.patches[uid] = patch\n",
    "            \n",
    "            \n",
    "    # Compute a distance matrix for the depth values of each well using Key Points\n",
    "    def computeTvdDistance(self, tvd1, tvd2, keyPoints1, keyPoints2):\n",
    "        \n",
    "        return cdist(tvd1[keyPoints1].reshape(-1, 1), tvd2[keyPoints2].reshape(-1, 1))\n",
    "    \n",
    "    \n",
    "    # Compute a distance matrix for the vector values of each well using Key Points\n",
    "    def computeVectorDistance(self, vector1, vector2, keyPoints1, keyPoints2, exp=1):\n",
    "        \n",
    "        return cdist(vector1[keyPoints1] ** exp, vector2[keyPoints2] ** exp, 'cosine')\n",
    "    \n",
    "    \n",
    "    # Set values of vector distance matrix outside of the allowable offset to INF\n",
    "    def computeVectorMask(self, vectorDist, tvdDist, exp=3):\n",
    "        \n",
    "        vectorMask = vectorDist.copy()\n",
    "        vectorMask[tvdDist > self.offset] = np.inf    \n",
    "        \n",
    "        tvdMask = ((tvdDist / self.offset) + 1) ** exp\n",
    "        \n",
    "        return vectorMask, tvdMask\n",
    "    \n",
    "    \n",
    "    # Compute the feature vector for specified well identifier\n",
    "    def getFeatureVector(self, uid):\n",
    "        \n",
    "        # Create a PyTorch tensor from the specified well identifier\n",
    "        tensor = torch.from_numpy(self.patches[uid]).float().cuda()\n",
    "        \n",
    "        # Pass the tensor through the models encode function\n",
    "        vector = self.model.encode(tensor).cpu().data.numpy()\n",
    "        \n",
    "        # Reshape vector\n",
    "        vector = vector.reshape(vector.shape[0], -1)\n",
    "        \n",
    "        return(vector)\n",
    "        \n",
    "        \n",
    "    # Find the key points from the provided dog along the specified axis\n",
    "    def getKeyPoints(self, dog, axis=1):\n",
    "        \n",
    "        # Get the peaks, then troughs from the dog along the specified axis\n",
    "        kp = argrelextrema(dog[:, axis], np.greater)[0].tolist()\n",
    "        kp += argrelextrema(dog[:, axis], np.less)[0].tolist()\n",
    "        kp.sort()\n",
    "        \n",
    "        return kp\n",
    "    \n",
    "    \n",
    "    # Function to automatically find well marker provided a source location\n",
    "    # This could be improved significantly\n",
    "    def getMarker(self, source):\n",
    "        \n",
    "        # Utility variables\n",
    "        push = heappush\n",
    "        pop = heappop    \n",
    "        c = count()\n",
    "        \n",
    "        # Placeholders \n",
    "        explored = []\n",
    "        queue = []\n",
    "        pick = []\n",
    "        \n",
    "        # Set up heap\n",
    "        push(queue, (next(c), source, 0)) # _, node, weight\n",
    "        \n",
    "        # Iterate over queue while there are still nodes that haven't been evaluated\n",
    "        while queue:\n",
    "            \n",
    "            # Extract the lowest weight node from the queue with its weight\n",
    "            _, curnode, weight = pop(queue)\n",
    "            \n",
    "            # If well has already been explored continue\n",
    "            if curnode[0] in explored:\n",
    "                continue\n",
    "            \n",
    "            # Add well to explored list and append node to pick\n",
    "            explored.append(curnode[0])\n",
    "            pick.append(curnode)\n",
    "            \n",
    "            # Find edges of curnode and add to heap\n",
    "            for k,v in self.G[curnode].items():\n",
    "                push(queue, (next(c), k, v['weight']))\n",
    "                \n",
    "        return(pick)\n",
    "    \n",
    "    \n",
    "    # Find matching points in the vector mask\n",
    "    def getMatchPoints(self, vectorMask):\n",
    "        \n",
    "        # Find the indices of the minimum value along the 0 axis\n",
    "        iPts = np.dstack((np.arange(vectorMask.shape[1]), \n",
    "                          vectorMask.argmin(axis=0)))[0]\n",
    "        \n",
    "        # Find the indices of the minimum value along the 1 axis\n",
    "        jPts = np.dstack((vectorMask.argmin(axis=1), \n",
    "                          np.arange(vectorMask.shape[0])))[0]\n",
    "        \n",
    "        # Stack points together, identify points that occurr multiple times\n",
    "        pts = np.vstack((iPts, jPts))\n",
    "        uni = np.unique(pts, axis=0, return_counts=True)\n",
    "        uni = uni[0][uni[1] > 1]\n",
    "        \n",
    "        # Pass points through RANSAC to remove outlier points\n",
    "        reg = RANSACRegressor(residual_threshold=2,\n",
    "                              min_samples=np.ceil(uni.shape[0] * 0.75))\n",
    "        reg.fit(uni[:, 0].reshape(-1, 1), uni[:, 1].reshape(-1,1))\n",
    "        \n",
    "        return uni[reg.inlier_mask_]\n",
    "    \n",
    "    \n",
    "    # Load the model\n",
    "    def getModel(self):\n",
    "        \n",
    "        self.model = ConvAEDeep().cuda()\n",
    "        self.model.load_state_dict(torch.load(self.modelPath))\n",
    "    \n",
    "    \n",
    "    # Load the well data\n",
    "    def getWellData(self):\n",
    "        \n",
    "        # Open the HDF5 store\n",
    "        with pd.HDFStore(self.projPath) as store:\n",
    "            \n",
    "            # Retrieve the header data\n",
    "            self.coords = store['header'].loc[:, ['X', 'Y']]\n",
    "            \n",
    "            # Iterate through the well identifiers, load log data, filter by depth,\n",
    "            # check the lenth of the log, resample, smooth, and add to list\n",
    "            logData = []\n",
    "            for uid in self.coords.index.tolist():\n",
    "                data = store['/log/{}'.format(uid)]\n",
    "                data['uid'] = uid\n",
    "                \n",
    "                if self.minZ is not None and self.maxZ is not None:\n",
    "                    idx = np.logical_and(data.index > self.minZ, data.index < self.maxZ)\n",
    "                    data = data.loc[idx]\n",
    "                elif self.minZ is not None:\n",
    "                    data = data[data.index > self.minZ]\n",
    "                elif self.maxZ is not None:\n",
    "                    data = data[data.index < self.maxZ] \n",
    "                    \n",
    "                if len(data) > 50:\n",
    "                    data = self.resampleLog(data)\n",
    "                    data = self.smoothLog(data)\n",
    "                    \n",
    "                    logData = self.logData.append(data)\n",
    "                \n",
    "            # Create a \"master\" log data data frame\n",
    "            self.logData = pd.concat(logData, ignore_index=True)\n",
    "            \n",
    "            \n",
    "    # Identify pairs of wells based on proximity to each other\n",
    "    def getWellPairs(self):\n",
    "        \n",
    "        # Create a distance matrix between wells\n",
    "        dist = cdist(self.coords.values, self.coords.values)\n",
    "        \n",
    "        # Iteratve over distance matrix, find wells that aren't neighbors based\n",
    "        # on the numNeighbor variable, and set to nan\n",
    "        for i in range(dist.shape[0]):\n",
    "            row = dist[i]\n",
    "            dist[i, row.argsort()[self.numNeighbors:]] = np.nan\n",
    "            \n",
    "        # Normalize the distance\n",
    "        dist = (dist - np.nanmin(dist)) / (np.nanmax(dist) - np.nanmin(dist))\n",
    "        \n",
    "        # Create a NetworkX graph from distance matrix and assign well identifiers\n",
    "        # as well node labels\n",
    "        self.wellPairs = nx.from_numpy_array(dist)\n",
    "        mapping = dict(zip(self.wellPairs.nodes(), self.coords.index))\n",
    "        self.wellPairs = nx.relabel_nodes(self.wellPairs, mapping)\n",
    "        \n",
    "        # Remove edges that are nan\n",
    "        remove = [(u,v) for u,v,w in self.wellPairs.edges(data=True) if np.isnan(w['weight'])]\n",
    "        self.wellPairs.remove_edges_from(remove)\n",
    "        \n",
    "        \n",
    "    # Resample the log values to a common increment\n",
    "    def resampleLog(self, logData):\n",
    "        \n",
    "        # Get log name and create a placeholder data frame\n",
    "        logName = logData.columns[0]\n",
    "        newLogData = pd.DataFrame(columns=self.logData.columns)\n",
    "        \n",
    "        # Extract depth and log values\n",
    "        tvd = logData.index.values\n",
    "        log = logData[logName].values\n",
    "        \n",
    "        # Create an interpolation function mapping depth to log values\n",
    "        func = interp1d(tvd, log)\n",
    "        \n",
    "        # Create resampled depth and log values\n",
    "        tvdNew = np.arange(np.ceil(tvd.min()), np.floor(tvd.max()), self.resampleInc)\n",
    "        logNew = func(tvdNew)\n",
    "        \n",
    "        # Assign placeholder data frame new values\n",
    "        newLogData[logName] = logNew\n",
    "        newLogData.index = tvdNew\n",
    "        \n",
    "        return newLogData\n",
    "    \n",
    "    \n",
    "    # Scale the log data\n",
    "    def scaleLogData(self):\n",
    "        \n",
    "        # Get log name and clip log data to P1 and P99\n",
    "        logName = self.logData.columns[0]\n",
    "        self.logData[logName] = np.clip(self.logData[logName], \n",
    "                                        np.percentile(self.logData[logName], 1), \n",
    "                                        np.percentile(self.logData[logName], 99))\n",
    "        \n",
    "        # Remove the mean and divide by std\n",
    "        stats = self.logData[logName].describe()\n",
    "        self.logData[logName] -= stats.loc['mean']\n",
    "        self.logData[logName] /= stats.loc['std']\n",
    "        stats = self.logData[logName].describe()\n",
    "        \n",
    "        # Scale between 0 and 1\n",
    "        self.logData[logName] -= stats.loc['min']\n",
    "        self.logData[logName] /= (stats.loc['max'] - stats.loc['min'])\n",
    "            \n",
    "            \n",
    "    # Smooth the specified log data\n",
    "    def smoothLog(self, logData):\n",
    "        \n",
    "        # Retrieve log name and if the smooth value is not None apply smoothing\n",
    "        logName = logData.columns[0]\n",
    "        if self.smooth is not None:\n",
    "            logData[logName] = gaussian_filter(logData[logName].values, self.smooth)\n",
    "            \n",
    "        return logData"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nazmi-py37",
   "language": "python",
   "name": "nazmi-py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
